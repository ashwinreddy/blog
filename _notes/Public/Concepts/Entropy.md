---
title: "Entropy"
feed: hide
---

Entropy is a measure of disorder. It is a term that is used both in information theory and in thermodynamics, but the mathematical form is similar.

## Shannon entropy

It is the expected Shannon [[Information]] of a random variable.[^base]

[^base]: This [[Abstraction]] hides a choice of logarithmic base, but in practice this matters little.

$$
H(X) = \mathbb{E}_X\left[I(x)\right]
$$

Since information is non-negative, entropy is non-negative.

## Thermodynamics

---